pkgbase = python-llama-cpp-cuda-git
	pkgdesc = Python bindings for llama.cpp (git version with CUDA support)
	pkgver = 0.3.16.r2070.8dcdbd2
	pkgrel = 1
	url = https://github.com/inference-sh/llama-cpp-python
	arch = x86_64
	license = MIT
	checkdepends = python-pytest
	checkdepends = python-huggingface-hub
	checkdepends = python-scipy
	checkdepends = python-httpx
	checkdepends = python-fastapi
	checkdepends = python-sse-starlette
	checkdepends = python-pydantic-settings
	makedepends = git
	makedepends = python-scikit-build
	makedepends = python-installer
	makedepends = python-build
	makedepends = python-wheel
	makedepends = python-scikit-build-core
	makedepends = gcc14
	depends = python-typing_extensions
	depends = python-numpy
	depends = python-diskcache
	depends = cuda
	depends = nvidia-utils
	depends = python-transformers
	depends = python-jinja
	depends = python-huggingface-hub
	depends = python-requests
	depends = python-openai
	optdepends = uvicorn
	optdepends = python-fastapi
	optdepends = python-pydantic-settings
	optdepends = python-sse-starlette
	optdepends = python-pyaml
	provides = python-llama-cpp
	provides = python-llama-cpp-cuda
	conflicts = python-llama-cpp
	conflicts = python-llama-cpp-cuda
	source = llama-cpp-python::git+https://github.com/inference-sh/llama-cpp-python.git
	sha256sums = SKIP

pkgname = python-llama-cpp-cuda-git
