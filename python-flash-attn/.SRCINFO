pkgbase = python-flash-attn
	pkgdesc = Fast and memory-efficient exact attention
	pkgver = 2.8.3
	pkgrel = 1
	url = https://github.com/Dao-AILab/flash-attention
	arch = x86_64
	license = BSD-3-Clause
	makedepends = cuda
	makedepends = git
	makedepends = ninja
	makedepends = python-build
	makedepends = python-packaging
	makedepends = python-psutil
	makedepends = python-installer
	makedepends = python-setuptools
	makedepends = python-wheel
	depends = python-pytorch-cuda
	depends = python-einops
	provides = python-flash-attention
	source = flash-attention::git+https://github.com/Dao-AILab/flash-attention.git#tag=v2.8.3
	source = 0001-fix-building-torch-extension-with-glog.patch
	sha256sums = SKIP
	sha256sums = cfdc7f13c855599e90861fcfc15ae5d3759b8823ca609a70ddbd061691287137

pkgname = python-flash-attn
