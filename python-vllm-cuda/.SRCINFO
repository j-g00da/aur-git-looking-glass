pkgbase = python-vllm-cuda
	pkgdesc = high-throughput and memory-efficient inference and serving engine for LLMs
	pkgver = 0.7.2
	pkgrel = 3
	url = https://github.com/vllm-project/vllm
	arch = x86_64
	license = Apache-2.0
	makedepends = git
	makedepends = gcc13
	makedepends = cuda
	makedepends = cuda-tools
	makedepends = python-setuptools-scm
	depends = python-installer
	depends = python
	depends = python-pytorch
	provides = python-vllm
	conflicts = python-vllm
	source = git+https://github.com/vllm-project/vllm.git#tag=v0.7.2
	sha256sums = 2b500afb4c0d192059d1c07f883a707f4d7d5fca82fe3aab3fc39ba4ccb36d69

pkgname = python-vllm-cuda
